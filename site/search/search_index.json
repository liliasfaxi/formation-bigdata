{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Formation Big Data \u00b6 Overview \u00b6 Ces travaux pratiques permettront de manipuler deux des technologies phares de l'\u00e9cosyst\u00e8me Big Data: Hadoop et Spark. Ils seront r\u00e9partis comme suit: Partie 1 : Hadoop HDFS Partie 2 : Hadoop Map Reduce Partie 3 : Tratement par Lot avec Spark Partie 4 : Tratement en Streaming avec Spark Outils et Versions \u00b6 Apache Hadoop Version: 3.3.6. Apache Spark Version: 3.5.0 Docker Version latest Visual Studio Code Version 1.85.1 (ou tout autre IDE de votre choix) Java Version 1.8. Unix-like ou Unix-based Systems (Divers Linux et MacOS) Lilia SFAXI Ce(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la Licence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International .","title":"Home"},{"location":"#formation-big-data","text":"","title":"Formation Big Data"},{"location":"#overview","text":"Ces travaux pratiques permettront de manipuler deux des technologies phares de l'\u00e9cosyst\u00e8me Big Data: Hadoop et Spark. Ils seront r\u00e9partis comme suit: Partie 1 : Hadoop HDFS Partie 2 : Hadoop Map Reduce Partie 3 : Tratement par Lot avec Spark Partie 4 : Tratement en Streaming avec Spark","title":"Overview"},{"location":"#outils-et-versions","text":"Apache Hadoop Version: 3.3.6. Apache Spark Version: 3.5.0 Docker Version latest Visual Studio Code Version 1.85.1 (ou tout autre IDE de votre choix) Java Version 1.8. Unix-like ou Unix-based Systems (Divers Linux et MacOS) Lilia SFAXI Ce(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la Licence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International .","title":"Outils et Versions"},{"location":"tp1/","text":"Objectifs \u00b6 Initiation au framework hadoop et manipulation de HDFS, utilisation de docker pour lancer un cluster hadoop de 3 noeuds. Hadoop \u00b6 Pr\u00e9sentation \u00b6 Apache Hadoop est un framework open-source pour stocker et traiter les donne\u0301es volumineuses sur un cluster. Il est utilise\u0301 par un grand nombre de contributeurs et utilisateurs. Il a une licence Apache 2.0. Hadoop et Docker \u00b6 Pour d\u00e9ployer le framework Hadoop, nous allons utiliser des contenaires Docker . L'utilisation des contenaires va garantir la consistance entre les environnements de d\u00e9veloppement et permettra de r\u00e9duire consid\u00e9rablement la complexit\u00e9 de configuration des machines (dans le cas d'un acc\u00e8s natif) ainsi que la lourdeur d'ex\u00e9cution (si on opte pour l'utilisation d'une machine virtuelle). Installation \u00b6 Nous allons utiliser trois contenaires repr\u00e9sentant respectivement un noeud ma\u00eetre (Namenode) et deux noeuds workers (Datanodes). Vous devez pour cela avoir install\u00e9 docker sur votre machine, et l'avoir correctement configur\u00e9. Ensuite, ouvrir la ligne de commande, et taper les instructions suivantes: T\u00e9l\u00e9charger l'image docker upload\u00e9e sur dockerhub: docker pull liliasfaxi/my-hadoop-spark:latest Cr\u00e9er les trois contenaires \u00e0 partir de l'image t\u00e9l\u00e9charg\u00e9e. Pour cela: 2.1. Cr\u00e9er un r\u00e9seau qui permettra de relier les trois contenaires: docker network create --driver = bridge hadoop 2.2. Cr\u00e9er et lancer les trois contenaires (les instructions -p permettent de faire un mapping entre les ports de la machine h\u00f4te et ceux du contenaire): Contenaire Master: docker run -itd --net = hadoop -p 9870 :9870 -p 8088 :8088 -p 7077 :7077 -p 16010 :16010 --name hadoop-master --hostname hadoop-master liliasfaxi/my-hadoop-spark:latest Contenaire Worker 1 docker run -itd -p 8040 :8042 --net = hadoop --name hadoop-worker1 --hostname hadoop-worker1 liliasfaxi/my-hadoop-spark:latest Contenaire Worker 2 docker run -itd -p 8041 :8042 --net = hadoop --name hadoop-worker2 --hostname hadoop-worker2 liliasfaxi/my-hadoop-spark:latest 2.3. V\u00e9rifier que les trois contenaires tournent bien en lan\u00e7ant la commande docker ps . Un r\u00e9sultat semblable au suivant devra s'afficher: Entrer dans le contenaire master pour commencer \u00e0 l'utiliser. docker exec -it hadoop-master bash Le r\u00e9sultat de cette ex\u00e9cution sera le suivant: root@hadoop-master:~# Vous vous retrouverez dans le shell du namenode, et vous pourrez ainsi manipuler le cluster \u00e0 votre guise. La premi\u00e8re chose \u00e0 faire, une fois dans le contenaire, est de lancer hadoop et yarn. Un script est fourni pour cela, appel\u00e9 start-hadoop.sh . Lancer ce script. ./start-hadoop.sh Le r\u00e9sultat devra ressembler \u00e0 ce qui suit: Vous pouvez visualiser les processus de HDFS et YARN qui tournent sur votre syst\u00e8me en lan\u00e7ant la commande jps , qui permet de lister les machines virtuelles Java (JVM) intrument\u00e9e sur le syst\u00e8me cible. Le r\u00e9sultat que vous devriez obtenir est le suivant: Ex\u00e9cuter la m\u00eame instruction sur une machine worker donnera le r\u00e9sultat suivant: On peut voir que les d\u00e9mons Namenode et Secondary Namenode de HDFS, ainsi que le Resource Manager de YARN tournent sur le master, alors que les d\u00e9mons Datanode de HDFS et NodeManager de YARN tournent sur le worker. Premiers pas avec Hadoop \u00b6 Toutes les commandes interagissant avec le syste\u0300me HDFS commencent par hdfs dfs . Ensuite, les options rajoute\u0301es sont tre\u0300s largement inspire\u0301es des commandes Unix standard. Cre\u0301er un re\u0301pertoire dans HDFS, appele\u0301 input . Pour cela, taper: hdfs dfs -mkdir -p input En cas d'erreur: No such file or directory Si pour une raison ou une autre, vous n'arrivez pas \u00e0 cr\u00e9er le r\u00e9pertoire input , avec un message ressemblant \u00e0 ceci: ls: `.': No such file or directory , veiller \u00e0 construire l'arborescence de l'utilisateur principal (root), comme suit: hdfs dfs -mkdir -p /user/root Nous allons utiliser le fichier purchases.txt pour manipuler hdfs. Commencer par t\u00e9l\u00e9charger le fichier sur votre propre machine, le d\u00e9compresser, puis par le charger dans le contenaire hadoop-master avec la commande suivante (\u00e0 ex\u00e9cuter dans le terminal de votre machine h\u00f4te): docker cp purchases.txt hadoop-master:/root/purchases.txt \u00c0 partir du contenaire master, charger le fichier purchases dans le r\u00e9pertoire input (de HDFS) que vous avez cr\u00e9\u00e9: hdfs dfs -put purchases.txt input Pour afficher le contenu du re\u0301pertoire input , la commande est: hdfs dfs -ls input Pour afficher les derni\u00e8res lignes du fichier purchases: hdfs dfs -tail input/purchases.txt Le r\u00e9sultat suivant va donc s'afficher: Nous pr\u00e9sentons dans le tableau suivant les commandes les plus utilis\u00e9es pour manipuler les fichiers dans HDFS: Instruction Fonctionnalit\u00e9 hdfs dfs \u2013ls Afficher le contenu du re\u0301pertoire racine hdfs dfs \u2013put file.txt Upload un fichier dans hadoop (a\u0300 partir du re\u0301pertoire courant de votre disque local) hdfs dfs \u2013get file.txt Download un fichier a\u0300 partir de hadoop sur votre disque local hdfs dfs \u2013tail file.txt Lire les dernie\u0300res lignes du fichier hdfs dfs \u2013cat file.txt Affiche tout le contenu du fichier hdfs dfs \u2013mv file.txt newfile.txt Renommer (ou d\u00e9placer) le fichier hdfs dfs \u2013rm newfile.txt Supprimer le fichier hdfs dfs \u2013mkdir myinput Cre\u0301er un re\u0301pertoire Interfaces web pour Hadoop \u00b6 Hadoop offre plusieurs interfaces web pour pouvoir observer le comportement de ses diff\u00e9rentes composantes. Il est possible d'afficher ces pages directement sur notre machine h\u00f4te, et ce gr\u00e2ce \u00e0 l'utilisation de l'option -p de la commande docker run . En effet, cette option permet de publier un port du contenaire sur la machine h\u00f4te. Pour pouvoir publier tous les ports expos\u00e9s, vous pouvez lancer votre contenaire en utilisant l'option -P . En regardant la commande docker run utilis\u00e9e plus haut, vous verrez que deux ports de la machine ma\u00eetre ont \u00e9t\u00e9 expos\u00e9s: Le port 9870 : qui permet d'afficher les informations de votre namenode. Le port 8088 : qui permet d'afficher les informations du resource manager de Yarn et visualiser le comportement des diff\u00e9rents jobs. Une fois votre cluster lanc\u00e9 et hadoop d\u00e9marr\u00e9 et pr\u00eat \u00e0 l'emploi, vous pouvez, sur votre navigateur pr\u00e9f\u00e9r\u00e9 de votre machine h\u00f4te, aller \u00e0 : http://localhost:9870 . Vous obtiendrez le r\u00e9sultat suivant: \u00c0 partir de cette interface, il est possible de voir le contenu de votre syst\u00e8me de fichier, en cliquant sur Utilities -> Browse the File System . L'interface affich\u00e9e sera la suivante: Vous pouvez \u00e9galement visualiser l'avancement et les r\u00e9sultats de vos Jobs (Map Reduce ou autre) en allant \u00e0 l'adresse: http://localhost:8088 .","title":"Partie 1 - Hadoop HDFS"},{"location":"tp1/#objectifs","text":"Initiation au framework hadoop et manipulation de HDFS, utilisation de docker pour lancer un cluster hadoop de 3 noeuds.","title":"Objectifs"},{"location":"tp1/#hadoop","text":"","title":"Hadoop"},{"location":"tp1/#presentation","text":"Apache Hadoop est un framework open-source pour stocker et traiter les donne\u0301es volumineuses sur un cluster. Il est utilise\u0301 par un grand nombre de contributeurs et utilisateurs. Il a une licence Apache 2.0.","title":"Pr\u00e9sentation"},{"location":"tp1/#hadoop-et-docker","text":"Pour d\u00e9ployer le framework Hadoop, nous allons utiliser des contenaires Docker . L'utilisation des contenaires va garantir la consistance entre les environnements de d\u00e9veloppement et permettra de r\u00e9duire consid\u00e9rablement la complexit\u00e9 de configuration des machines (dans le cas d'un acc\u00e8s natif) ainsi que la lourdeur d'ex\u00e9cution (si on opte pour l'utilisation d'une machine virtuelle).","title":"Hadoop et Docker"},{"location":"tp1/#installation","text":"Nous allons utiliser trois contenaires repr\u00e9sentant respectivement un noeud ma\u00eetre (Namenode) et deux noeuds workers (Datanodes). Vous devez pour cela avoir install\u00e9 docker sur votre machine, et l'avoir correctement configur\u00e9. Ensuite, ouvrir la ligne de commande, et taper les instructions suivantes: T\u00e9l\u00e9charger l'image docker upload\u00e9e sur dockerhub: docker pull liliasfaxi/my-hadoop-spark:latest Cr\u00e9er les trois contenaires \u00e0 partir de l'image t\u00e9l\u00e9charg\u00e9e. Pour cela: 2.1. Cr\u00e9er un r\u00e9seau qui permettra de relier les trois contenaires: docker network create --driver = bridge hadoop 2.2. Cr\u00e9er et lancer les trois contenaires (les instructions -p permettent de faire un mapping entre les ports de la machine h\u00f4te et ceux du contenaire): Contenaire Master: docker run -itd --net = hadoop -p 9870 :9870 -p 8088 :8088 -p 7077 :7077 -p 16010 :16010 --name hadoop-master --hostname hadoop-master liliasfaxi/my-hadoop-spark:latest Contenaire Worker 1 docker run -itd -p 8040 :8042 --net = hadoop --name hadoop-worker1 --hostname hadoop-worker1 liliasfaxi/my-hadoop-spark:latest Contenaire Worker 2 docker run -itd -p 8041 :8042 --net = hadoop --name hadoop-worker2 --hostname hadoop-worker2 liliasfaxi/my-hadoop-spark:latest 2.3. V\u00e9rifier que les trois contenaires tournent bien en lan\u00e7ant la commande docker ps . Un r\u00e9sultat semblable au suivant devra s'afficher: Entrer dans le contenaire master pour commencer \u00e0 l'utiliser. docker exec -it hadoop-master bash Le r\u00e9sultat de cette ex\u00e9cution sera le suivant: root@hadoop-master:~# Vous vous retrouverez dans le shell du namenode, et vous pourrez ainsi manipuler le cluster \u00e0 votre guise. La premi\u00e8re chose \u00e0 faire, une fois dans le contenaire, est de lancer hadoop et yarn. Un script est fourni pour cela, appel\u00e9 start-hadoop.sh . Lancer ce script. ./start-hadoop.sh Le r\u00e9sultat devra ressembler \u00e0 ce qui suit: Vous pouvez visualiser les processus de HDFS et YARN qui tournent sur votre syst\u00e8me en lan\u00e7ant la commande jps , qui permet de lister les machines virtuelles Java (JVM) intrument\u00e9e sur le syst\u00e8me cible. Le r\u00e9sultat que vous devriez obtenir est le suivant: Ex\u00e9cuter la m\u00eame instruction sur une machine worker donnera le r\u00e9sultat suivant: On peut voir que les d\u00e9mons Namenode et Secondary Namenode de HDFS, ainsi que le Resource Manager de YARN tournent sur le master, alors que les d\u00e9mons Datanode de HDFS et NodeManager de YARN tournent sur le worker.","title":"Installation"},{"location":"tp1/#premiers-pas-avec-hadoop","text":"Toutes les commandes interagissant avec le syste\u0300me HDFS commencent par hdfs dfs . Ensuite, les options rajoute\u0301es sont tre\u0300s largement inspire\u0301es des commandes Unix standard. Cre\u0301er un re\u0301pertoire dans HDFS, appele\u0301 input . Pour cela, taper: hdfs dfs -mkdir -p input En cas d'erreur: No such file or directory Si pour une raison ou une autre, vous n'arrivez pas \u00e0 cr\u00e9er le r\u00e9pertoire input , avec un message ressemblant \u00e0 ceci: ls: `.': No such file or directory , veiller \u00e0 construire l'arborescence de l'utilisateur principal (root), comme suit: hdfs dfs -mkdir -p /user/root Nous allons utiliser le fichier purchases.txt pour manipuler hdfs. Commencer par t\u00e9l\u00e9charger le fichier sur votre propre machine, le d\u00e9compresser, puis par le charger dans le contenaire hadoop-master avec la commande suivante (\u00e0 ex\u00e9cuter dans le terminal de votre machine h\u00f4te): docker cp purchases.txt hadoop-master:/root/purchases.txt \u00c0 partir du contenaire master, charger le fichier purchases dans le r\u00e9pertoire input (de HDFS) que vous avez cr\u00e9\u00e9: hdfs dfs -put purchases.txt input Pour afficher le contenu du re\u0301pertoire input , la commande est: hdfs dfs -ls input Pour afficher les derni\u00e8res lignes du fichier purchases: hdfs dfs -tail input/purchases.txt Le r\u00e9sultat suivant va donc s'afficher: Nous pr\u00e9sentons dans le tableau suivant les commandes les plus utilis\u00e9es pour manipuler les fichiers dans HDFS: Instruction Fonctionnalit\u00e9 hdfs dfs \u2013ls Afficher le contenu du re\u0301pertoire racine hdfs dfs \u2013put file.txt Upload un fichier dans hadoop (a\u0300 partir du re\u0301pertoire courant de votre disque local) hdfs dfs \u2013get file.txt Download un fichier a\u0300 partir de hadoop sur votre disque local hdfs dfs \u2013tail file.txt Lire les dernie\u0300res lignes du fichier hdfs dfs \u2013cat file.txt Affiche tout le contenu du fichier hdfs dfs \u2013mv file.txt newfile.txt Renommer (ou d\u00e9placer) le fichier hdfs dfs \u2013rm newfile.txt Supprimer le fichier hdfs dfs \u2013mkdir myinput Cre\u0301er un re\u0301pertoire","title":"Premiers pas avec Hadoop"},{"location":"tp1/#interfaces-web-pour-hadoop","text":"Hadoop offre plusieurs interfaces web pour pouvoir observer le comportement de ses diff\u00e9rentes composantes. Il est possible d'afficher ces pages directement sur notre machine h\u00f4te, et ce gr\u00e2ce \u00e0 l'utilisation de l'option -p de la commande docker run . En effet, cette option permet de publier un port du contenaire sur la machine h\u00f4te. Pour pouvoir publier tous les ports expos\u00e9s, vous pouvez lancer votre contenaire en utilisant l'option -P . En regardant la commande docker run utilis\u00e9e plus haut, vous verrez que deux ports de la machine ma\u00eetre ont \u00e9t\u00e9 expos\u00e9s: Le port 9870 : qui permet d'afficher les informations de votre namenode. Le port 8088 : qui permet d'afficher les informations du resource manager de Yarn et visualiser le comportement des diff\u00e9rents jobs. Une fois votre cluster lanc\u00e9 et hadoop d\u00e9marr\u00e9 et pr\u00eat \u00e0 l'emploi, vous pouvez, sur votre navigateur pr\u00e9f\u00e9r\u00e9 de votre machine h\u00f4te, aller \u00e0 : http://localhost:9870 . Vous obtiendrez le r\u00e9sultat suivant: \u00c0 partir de cette interface, il est possible de voir le contenu de votre syst\u00e8me de fichier, en cliquant sur Utilities -> Browse the File System . L'interface affich\u00e9e sera la suivante: Vous pouvez \u00e9galement visualiser l'avancement et les r\u00e9sultats de vos Jobs (Map Reduce ou autre) en allant \u00e0 l'adresse: http://localhost:8088 .","title":"Interfaces web pour Hadoop"},{"location":"tp2/","text":"Objectifs \u00b6 Initiation \u00e0 Hadoop Map Reduce. Utilisation d'un code pr\u00e9d\u00e9fini et manipulation de l'API avec Java. Pr\u00e9sentation de Map Reduce \u00b6 Un Job Map-Reduce se compose principalement de deux types de programmes: Mappers : permettent d\u2019extraire les donne\u0301es ne\u0301cessaires sous forme de clef/valeur, pour pouvoir ensuite les trier selon la clef Reducers : prennent un ensemble de donne\u0301es trie\u0301es selon leur clef, et effectuent le traitement ne\u0301cessaire sur ces donne\u0301es (somme, moyenne, total...) Wordcount \u00b6 Nous allons tester un programme MapReduce gr\u00e2ce \u00e0 un exemple tr\u00e8s simple, le WordCount , l'\u00e9quivalent du HelloWorld pour les applications de traitement de donn\u00e9es. Le Wordcount permet de calculer le nombre de mots dans un fichier donn\u00e9, en d\u00e9composant le calcul en deux \u00e9tapes: L'\u00e9tape de Mapping , qui permet de d\u00e9couper le texte en mots et de d\u00e9livrer en sortie un flux textuel, o\u00f9 chaque ligne contient le mot trouv\u00e9, suivi de la valeur 1 (pour dire que le mot a \u00e9t\u00e9 trouv\u00e9 une fois) L'\u00e9tape de Reducing , qui permet de faire la somme des 1 pour chaque mot, pour trouver le nombre total d'occurrences de ce mot dans le texte. Tester Wordcount \u00b6 Nous commen\u00e7ons par tester la bonne ex\u00e9cution d'un code Map Reduce pr\u00e9d\u00e9fini, en faisant appel \u00e0 un exemple de wordcount fourni par le framework Hadoop. Pour cela: Commencer par cr\u00e9er un r\u00e9pertoire test dans votre contenaire master: mkdir test Cr\u00e9er deux fichiers file1 et file2 dans test contenant chacun une ligne de texte, comme suit: echo \"Hello Docker\" >test/file2.txt echo \"Hello Hadoop\" >test/file1.txt Cr\u00e9er un r\u00e9pertoire test dans HDFS, et y charger les deux fichiers: hdfs dfs -mkdir test hdfs dfs -put ./test/* test Lancer le job Wordcount en faisant appel au fichier jar pr\u00e9d\u00e9fini de Hadoop. Le r\u00e9sultat sera enregistr\u00e9 dans le r\u00e9pertoire outest de HDFS: hadoop jar $HADOOP_HOME /share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-3.3.6-sources.jar org.apache.hadoop.examples.WordCount test outest Le job va se lancer, en affichant plusieurs lignes sur le terminal. On pourra visualiser le r\u00e9sultat en tapant les lignes suivantes: # Afficher les fichiers en entr\u00e9e echo -e \"\\nEntr\u00e9e - file1.txt:\" hdfs dfs -cat test/file1.txt echo -e \"\\nEntr\u00e9e - file2.txt:\" hdfs dfs -cat test/file2.txt # afficher le r\u00e9sultat echo -e \"\\nSortie:\" hdfs dfs -cat outest/part-r-00000 Le r\u00e9sultat sera le suivant: Wordcount avec Java \u00b6 Commen\u00e7ons par cr\u00e9er un projet Maven dans IntelliJ IDEA. Nous utiliserons dans notre cas JDK 1.8 . Version de JDK Ceci n'est pas une suggestion: l'utilisation d'une autre version que 1.8 provoquera des erreurs sans fin. Hadoop est compil\u00e9 avec cette version de Java, connue pour sa stabilit\u00e9. Pour cr\u00e9er un projet Maven dans IntelliJIDEA: Cr\u00e9er un nouveau projet en cliquant sur Maven dans la fen\u00eatre de gauche, et en choisissant le SDK 1.8 dans la fen\u00eatre d\u00e9roulante. Nommer votre projet 1-Hadoop-MapReduce , et d\u00e9finir les valeurs suivantes dans la partie Artifact Coordinates : GroupId : hadoop.mapreduce ArtifactId : wordcount Version : 1 Ouvrir le fichier pom.xml automatiquement cr\u00e9\u00e9, et remplacer son contenu par le code suivant: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> hadoop.mapreduce </groupId> <artifactId> wordcount </artifactId> <version> 1 </version> <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-common </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-core </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-hdfs </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-common --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-common </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-jobclient --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-jobclient </artifactId> <version> 3.3.6 </version> </dependency> </dependencies> </project> Cr\u00e9er un package mapreduce sous le r\u00e9pertoire src/main/java/ Cr\u00e9er la classe TokenizerMapper , contenant ce code: package mapreduce ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Mapper ; import java.io.IOException ; import java.util.StringTokenizer ; public class TokenizerMapper extends Mapper < Object , Text , Text , IntWritable > { private final static IntWritable one = new IntWritable ( 1 ); private Text word = new Text (); public void map ( Object key , Text value , Mapper . Context context ) throws IOException , InterruptedException { StringTokenizer itr = new StringTokenizer ( value . toString ()); while ( itr . hasMoreTokens ()) { word . set ( itr . nextToken ()); context . write ( word , one ); } } } La m\u00e9thode map manipule une ligne d'entr\u00e9e ( value ), commence par la diviser en mots gr\u00e2ce \u00e0 la classe StringTokenizer , ensuite, pour chaque mot, elle renvoie en sortie (dans l'object global context ) ce mot accompagn\u00e9 de la valeur 1 . Cr\u00e9er la classe IntSumReducer : package mapreduce ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Reducer ; import java.io.IOException ; public class IntSumReducer extends Reducer < Text , IntWritable , Text , IntWritable > { private IntWritable result = new IntWritable (); public void reduce ( Text key , Iterable < IntWritable > values , Context context ) throws IOException , InterruptedException { int sum = 0 ; for ( IntWritable val : values ) { System . out . println ( \"value: \" + val . get ()); sum += val . get (); } System . out . println ( \"--> Sum = \" + sum ); result . set ( sum ); context . write ( key , result ); } } La m\u00e9thode reduce re\u00e7oit en entr\u00e9e un couple clef/valeurs. En effet, Hadoop se charge, en arri\u00e8re plan dans son \u00e9tape de Shuffle and Sort de regrouper les \u00e9l\u00e9ments en sortie du Mapper, ayant la m\u00eame clef, ainsi que leurs diff\u00e9rentes valeurs. Par exemple, si le mot Bonjour existe sur deux lignes diff\u00e9rentes en sortie des Mappers, il arrive au reducer sous la forme : Bonjour 1 1 . Ce reducer se charge alors de parcourir les valeurs associ\u00e9s \u00e0 la m\u00eame clef, puis de faire leurs sommes. Enfin, cr\u00e9er la classe principale WordCount : package mapreduce ; import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Job ; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat ; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ; public class WordCount { public static void main ( String [] args ) throws Exception { Configuration conf = new Configuration (); Job job = Job . getInstance ( conf , \"word count\" ); job . setJarByClass ( WordCount . class ); job . setMapperClass ( TokenizerMapper . class ); job . setCombinerClass ( IntSumReducer . class ); job . setReducerClass ( IntSumReducer . class ); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ] )); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ] )); System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); } } Cette classe permet de lancer le Job en donnant les diff\u00e9rentes configurations. Par exemple, elle d\u00e9clare la classe IntSumReducer comme \u00e9tant \u00e0 la fois le reducer et le combiner du Job Map Reduce. Tester Map Reduce en local \u00b6 Dans votre projet sur IntelliJ: Cr\u00e9er un r\u00e9pertoire input sous le r\u00e9pertoire resources de votre projet. Cr\u00e9er un fichier de test: file.txt dans lequel vous ins\u00e8rerez les deux lignes: Hello Wordcount! Hello Hadoop! Nous allons maintenant d\u00e9finir des arguments \u00e0 la m\u00e9thode Main: le fichier en entr\u00e9e sur lequel Map reduce va travailler, et le r\u00e9pertoire en sortie dans lequel le r\u00e9sultat sera stock\u00e9. Pour cela: Cliquer sur le bouton Add Configuration... qui se trouve en haut \u00e0 droite de votre fen\u00eatre principale dans IntelliJ, ou alors aller vers le menu Run -> Edit Configurations... Cliquer sur le bouton + en haut \u00e0 gauche de la fen\u00eatre qui vient de s'ouvrir. Choisir Application dans la liste. Nommer votre configuration local-run Dans la partie Build and run , prenez soin de choisir la version 1.8 de Java Choisir mapreduce.WordCount comme \u00e9tant la Main Class \u00e0 ex\u00e9cuter. Dans le champ Program arguments , saisir les arguments suivants: src/main/resources/input/file.txt src/main/resources/output . Nous indiquons ainsi que le r\u00e9pertoire output est le r\u00e9pertoire de sortie de notre ex\u00e9cution. ATTENTION, output ne doit pas exister avant le lancement du job. Voici la fen\u00eatre de configuration finale: Lancer le programme. Un r\u00e9pertoire output sera cr\u00e9\u00e9 dans le r\u00e9pertoire resources , contenant notamment un fichier part-r-00000 , dont le contenu devrait \u00eatre le suivant: Hadoop! 1 Hello 2 Wordcount! 1 Lancer Map Reduce sur le cluster \u00b6 Dans votre projet IntelliJ: Pour pouvoir encapsuler toutes les d\u00e9pendances du projet dans le fichier JAR \u00e0 exporter, ajouter les lignes suivantes dans le fichier pom.xml de votre projet, juste apr\u00e8s les d\u00e9pendances: <build> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-assembly-plugin </artifactId> <version> 3.6.0 </version> <!-- Use latest version --> <configuration> <archive> <manifest> <mainClass> mapreduce.WordCount </mainClass> </manifest> </archive> <descriptorRefs> <descriptorRef> jar-with-dependencies </descriptorRef> </descriptorRefs> </configuration> <executions> <execution> <id> make-assembly </id> <!-- this is used for inheritance merges --> <phase> package </phase> <!-- bind to the packaging phase --> <goals> <goal> single </goal> </goals> </execution> </executions> </plugin> </plugins> </build> Cr\u00e9er une nouvelle configuration d'ex\u00e9cution, cette fois-ci de type Maven, qu'on appellera wordcount-jar , et qui va ex\u00e9cuter la commande package pour cr\u00e9er un nouveau fichier jar ex\u00e9cutable \u00e0 partir de notre code. La configuration sera comme suit: Lancer la configuration. Un fichier wordcount-1-jar-with-dependencies.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target du projet. Copier le fichier jar cr\u00e9\u00e9 dans le contenaire master. Pour cela: Ouvrir le terminal sur le r\u00e9pertoire du projet. Cela peut \u00eatre fait avec IntelliJ en ouvrant directement un terminal. Taper la commande suivante: docker cp target/wordcount-1-jar-with-dependencies.jar hadoop-master:/root/wordcount.jar Revenir au shell du contenaire master, et lancer le job map reduce avec cette commande: hadoop jar wordcount.jar input output Le Job sera lanc\u00e9 sur le fichier purchases.txt que vous aviez pr\u00e9alablement charg\u00e9 dans le r\u00e9pertoire input de HDFS. Une fois le Job termin\u00e9, un r\u00e9pertoire output sera cr\u00e9\u00e9. Si tout se passe bien, vous obtiendrez un affichage ressemblant au suivant: En affichant les derni\u00e8res lignes du fichier g\u00e9n\u00e9r\u00e9 output/part-r-00000 , avec hdfs dfs -tail output/part-r-00000 , vous obtiendrez l'affichage suivant: Il vous est possible de monitorer vos Jobs Map Reduce, en allant \u00e0 la page: http://localhost:8088 . Vous trouverez votre Job dans la liste des applications comme suit: Il est \u00e9galement possible de voir le comportement des noeuds workers, en allant \u00e0 l'adresse: http://localhost:8041 pour worker1 , et http://localhost:8042 pour worker2 . Vous obtiendrez ce qui suit:","title":"Partie 2 - Hadoop Map Reduce"},{"location":"tp2/#objectifs","text":"Initiation \u00e0 Hadoop Map Reduce. Utilisation d'un code pr\u00e9d\u00e9fini et manipulation de l'API avec Java.","title":"Objectifs"},{"location":"tp2/#presentation-de-map-reduce","text":"Un Job Map-Reduce se compose principalement de deux types de programmes: Mappers : permettent d\u2019extraire les donne\u0301es ne\u0301cessaires sous forme de clef/valeur, pour pouvoir ensuite les trier selon la clef Reducers : prennent un ensemble de donne\u0301es trie\u0301es selon leur clef, et effectuent le traitement ne\u0301cessaire sur ces donne\u0301es (somme, moyenne, total...)","title":"Pr\u00e9sentation de Map Reduce"},{"location":"tp2/#wordcount","text":"Nous allons tester un programme MapReduce gr\u00e2ce \u00e0 un exemple tr\u00e8s simple, le WordCount , l'\u00e9quivalent du HelloWorld pour les applications de traitement de donn\u00e9es. Le Wordcount permet de calculer le nombre de mots dans un fichier donn\u00e9, en d\u00e9composant le calcul en deux \u00e9tapes: L'\u00e9tape de Mapping , qui permet de d\u00e9couper le texte en mots et de d\u00e9livrer en sortie un flux textuel, o\u00f9 chaque ligne contient le mot trouv\u00e9, suivi de la valeur 1 (pour dire que le mot a \u00e9t\u00e9 trouv\u00e9 une fois) L'\u00e9tape de Reducing , qui permet de faire la somme des 1 pour chaque mot, pour trouver le nombre total d'occurrences de ce mot dans le texte.","title":"Wordcount"},{"location":"tp2/#tester-wordcount","text":"Nous commen\u00e7ons par tester la bonne ex\u00e9cution d'un code Map Reduce pr\u00e9d\u00e9fini, en faisant appel \u00e0 un exemple de wordcount fourni par le framework Hadoop. Pour cela: Commencer par cr\u00e9er un r\u00e9pertoire test dans votre contenaire master: mkdir test Cr\u00e9er deux fichiers file1 et file2 dans test contenant chacun une ligne de texte, comme suit: echo \"Hello Docker\" >test/file2.txt echo \"Hello Hadoop\" >test/file1.txt Cr\u00e9er un r\u00e9pertoire test dans HDFS, et y charger les deux fichiers: hdfs dfs -mkdir test hdfs dfs -put ./test/* test Lancer le job Wordcount en faisant appel au fichier jar pr\u00e9d\u00e9fini de Hadoop. Le r\u00e9sultat sera enregistr\u00e9 dans le r\u00e9pertoire outest de HDFS: hadoop jar $HADOOP_HOME /share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-3.3.6-sources.jar org.apache.hadoop.examples.WordCount test outest Le job va se lancer, en affichant plusieurs lignes sur le terminal. On pourra visualiser le r\u00e9sultat en tapant les lignes suivantes: # Afficher les fichiers en entr\u00e9e echo -e \"\\nEntr\u00e9e - file1.txt:\" hdfs dfs -cat test/file1.txt echo -e \"\\nEntr\u00e9e - file2.txt:\" hdfs dfs -cat test/file2.txt # afficher le r\u00e9sultat echo -e \"\\nSortie:\" hdfs dfs -cat outest/part-r-00000 Le r\u00e9sultat sera le suivant:","title":"Tester Wordcount"},{"location":"tp2/#wordcount-avec-java","text":"Commen\u00e7ons par cr\u00e9er un projet Maven dans IntelliJ IDEA. Nous utiliserons dans notre cas JDK 1.8 . Version de JDK Ceci n'est pas une suggestion: l'utilisation d'une autre version que 1.8 provoquera des erreurs sans fin. Hadoop est compil\u00e9 avec cette version de Java, connue pour sa stabilit\u00e9. Pour cr\u00e9er un projet Maven dans IntelliJIDEA: Cr\u00e9er un nouveau projet en cliquant sur Maven dans la fen\u00eatre de gauche, et en choisissant le SDK 1.8 dans la fen\u00eatre d\u00e9roulante. Nommer votre projet 1-Hadoop-MapReduce , et d\u00e9finir les valeurs suivantes dans la partie Artifact Coordinates : GroupId : hadoop.mapreduce ArtifactId : wordcount Version : 1 Ouvrir le fichier pom.xml automatiquement cr\u00e9\u00e9, et remplacer son contenu par le code suivant: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> hadoop.mapreduce </groupId> <artifactId> wordcount </artifactId> <version> 1 </version> <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-common </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-core </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-hdfs </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-common --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-common </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-jobclient --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-jobclient </artifactId> <version> 3.3.6 </version> </dependency> </dependencies> </project> Cr\u00e9er un package mapreduce sous le r\u00e9pertoire src/main/java/ Cr\u00e9er la classe TokenizerMapper , contenant ce code: package mapreduce ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Mapper ; import java.io.IOException ; import java.util.StringTokenizer ; public class TokenizerMapper extends Mapper < Object , Text , Text , IntWritable > { private final static IntWritable one = new IntWritable ( 1 ); private Text word = new Text (); public void map ( Object key , Text value , Mapper . Context context ) throws IOException , InterruptedException { StringTokenizer itr = new StringTokenizer ( value . toString ()); while ( itr . hasMoreTokens ()) { word . set ( itr . nextToken ()); context . write ( word , one ); } } } La m\u00e9thode map manipule une ligne d'entr\u00e9e ( value ), commence par la diviser en mots gr\u00e2ce \u00e0 la classe StringTokenizer , ensuite, pour chaque mot, elle renvoie en sortie (dans l'object global context ) ce mot accompagn\u00e9 de la valeur 1 . Cr\u00e9er la classe IntSumReducer : package mapreduce ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Reducer ; import java.io.IOException ; public class IntSumReducer extends Reducer < Text , IntWritable , Text , IntWritable > { private IntWritable result = new IntWritable (); public void reduce ( Text key , Iterable < IntWritable > values , Context context ) throws IOException , InterruptedException { int sum = 0 ; for ( IntWritable val : values ) { System . out . println ( \"value: \" + val . get ()); sum += val . get (); } System . out . println ( \"--> Sum = \" + sum ); result . set ( sum ); context . write ( key , result ); } } La m\u00e9thode reduce re\u00e7oit en entr\u00e9e un couple clef/valeurs. En effet, Hadoop se charge, en arri\u00e8re plan dans son \u00e9tape de Shuffle and Sort de regrouper les \u00e9l\u00e9ments en sortie du Mapper, ayant la m\u00eame clef, ainsi que leurs diff\u00e9rentes valeurs. Par exemple, si le mot Bonjour existe sur deux lignes diff\u00e9rentes en sortie des Mappers, il arrive au reducer sous la forme : Bonjour 1 1 . Ce reducer se charge alors de parcourir les valeurs associ\u00e9s \u00e0 la m\u00eame clef, puis de faire leurs sommes. Enfin, cr\u00e9er la classe principale WordCount : package mapreduce ; import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Job ; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat ; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ; public class WordCount { public static void main ( String [] args ) throws Exception { Configuration conf = new Configuration (); Job job = Job . getInstance ( conf , \"word count\" ); job . setJarByClass ( WordCount . class ); job . setMapperClass ( TokenizerMapper . class ); job . setCombinerClass ( IntSumReducer . class ); job . setReducerClass ( IntSumReducer . class ); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ] )); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ] )); System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); } } Cette classe permet de lancer le Job en donnant les diff\u00e9rentes configurations. Par exemple, elle d\u00e9clare la classe IntSumReducer comme \u00e9tant \u00e0 la fois le reducer et le combiner du Job Map Reduce.","title":"Wordcount avec Java"},{"location":"tp2/#tester-map-reduce-en-local","text":"Dans votre projet sur IntelliJ: Cr\u00e9er un r\u00e9pertoire input sous le r\u00e9pertoire resources de votre projet. Cr\u00e9er un fichier de test: file.txt dans lequel vous ins\u00e8rerez les deux lignes: Hello Wordcount! Hello Hadoop! Nous allons maintenant d\u00e9finir des arguments \u00e0 la m\u00e9thode Main: le fichier en entr\u00e9e sur lequel Map reduce va travailler, et le r\u00e9pertoire en sortie dans lequel le r\u00e9sultat sera stock\u00e9. Pour cela: Cliquer sur le bouton Add Configuration... qui se trouve en haut \u00e0 droite de votre fen\u00eatre principale dans IntelliJ, ou alors aller vers le menu Run -> Edit Configurations... Cliquer sur le bouton + en haut \u00e0 gauche de la fen\u00eatre qui vient de s'ouvrir. Choisir Application dans la liste. Nommer votre configuration local-run Dans la partie Build and run , prenez soin de choisir la version 1.8 de Java Choisir mapreduce.WordCount comme \u00e9tant la Main Class \u00e0 ex\u00e9cuter. Dans le champ Program arguments , saisir les arguments suivants: src/main/resources/input/file.txt src/main/resources/output . Nous indiquons ainsi que le r\u00e9pertoire output est le r\u00e9pertoire de sortie de notre ex\u00e9cution. ATTENTION, output ne doit pas exister avant le lancement du job. Voici la fen\u00eatre de configuration finale: Lancer le programme. Un r\u00e9pertoire output sera cr\u00e9\u00e9 dans le r\u00e9pertoire resources , contenant notamment un fichier part-r-00000 , dont le contenu devrait \u00eatre le suivant: Hadoop! 1 Hello 2 Wordcount! 1","title":"Tester Map Reduce en local"},{"location":"tp2/#lancer-map-reduce-sur-le-cluster","text":"Dans votre projet IntelliJ: Pour pouvoir encapsuler toutes les d\u00e9pendances du projet dans le fichier JAR \u00e0 exporter, ajouter les lignes suivantes dans le fichier pom.xml de votre projet, juste apr\u00e8s les d\u00e9pendances: <build> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-assembly-plugin </artifactId> <version> 3.6.0 </version> <!-- Use latest version --> <configuration> <archive> <manifest> <mainClass> mapreduce.WordCount </mainClass> </manifest> </archive> <descriptorRefs> <descriptorRef> jar-with-dependencies </descriptorRef> </descriptorRefs> </configuration> <executions> <execution> <id> make-assembly </id> <!-- this is used for inheritance merges --> <phase> package </phase> <!-- bind to the packaging phase --> <goals> <goal> single </goal> </goals> </execution> </executions> </plugin> </plugins> </build> Cr\u00e9er une nouvelle configuration d'ex\u00e9cution, cette fois-ci de type Maven, qu'on appellera wordcount-jar , et qui va ex\u00e9cuter la commande package pour cr\u00e9er un nouveau fichier jar ex\u00e9cutable \u00e0 partir de notre code. La configuration sera comme suit: Lancer la configuration. Un fichier wordcount-1-jar-with-dependencies.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target du projet. Copier le fichier jar cr\u00e9\u00e9 dans le contenaire master. Pour cela: Ouvrir le terminal sur le r\u00e9pertoire du projet. Cela peut \u00eatre fait avec IntelliJ en ouvrant directement un terminal. Taper la commande suivante: docker cp target/wordcount-1-jar-with-dependencies.jar hadoop-master:/root/wordcount.jar Revenir au shell du contenaire master, et lancer le job map reduce avec cette commande: hadoop jar wordcount.jar input output Le Job sera lanc\u00e9 sur le fichier purchases.txt que vous aviez pr\u00e9alablement charg\u00e9 dans le r\u00e9pertoire input de HDFS. Une fois le Job termin\u00e9, un r\u00e9pertoire output sera cr\u00e9\u00e9. Si tout se passe bien, vous obtiendrez un affichage ressemblant au suivant: En affichant les derni\u00e8res lignes du fichier g\u00e9n\u00e9r\u00e9 output/part-r-00000 , avec hdfs dfs -tail output/part-r-00000 , vous obtiendrez l'affichage suivant: Il vous est possible de monitorer vos Jobs Map Reduce, en allant \u00e0 la page: http://localhost:8088 . Vous trouverez votre Job dans la liste des applications comme suit: Il est \u00e9galement possible de voir le comportement des noeuds workers, en allant \u00e0 l'adresse: http://localhost:8041 pour worker1 , et http://localhost:8042 pour worker2 . Vous obtiendrez ce qui suit:","title":"Lancer Map Reduce sur le cluster"},{"location":"tp3/","text":"Objectifs \u00b6 Manipulation de Spark Shell avec Scala pour lancer un traitement sur des donn\u00e9es sur HDFS. Spark \u00b6 Pr\u00e9sentation \u00b6 Spark est un syst\u00e8me de traitement rapide et parall\u00e8le. Il fournit des APIs de haut niveau en Java, Scala, Python et R, et un moteur optimis\u00e9 qui supporte l'ex\u00e9cution des graphes. Il supporte \u00e9galement un ensemble d'outils de haut niveau tels que Spark SQL pour le support du traitement de donn\u00e9es structur\u00e9es, MLlib pour l'apprentissage des donn\u00e9es, GraphX pour le traitement des graphes, et Spark Streaming pour le traitment des donn\u00e9es en streaming. Spark et Hadoop \u00b6 Spark peut s'ex\u00e9cuter sur plusieurs plateformes: Hadoop, Mesos, en standalone ou sur le cloud. Il peut \u00e9galement acc\u00e9der \u00e0 diverses sources de donn\u00e9es, comme HDFS, Cassandra, HBase et S3. Dans ce lab, nous allons ex\u00e9cuter Spark sur Hadoop YARN. YARN s'occupera ainsi de la gestion des ressources pour le d\u00e9clenchement et l'ex\u00e9cution des Jobs Spark. Installation \u00b6 Nous avons proc\u00e9d\u00e9 \u00e0 l'installation de Spark sur le cluster Hadoop utilis\u00e9 dans le TP1 . Suivre les \u00e9tapes d\u00e9crites dans la partie Installation du TP1 pour t\u00e9l\u00e9charger l'image et ex\u00e9cuter les trois contenaires. Si cela est d\u00e9j\u00e0 fait, il suffit de lancer vos machines gr\u00e2ce aux commandes suivantes: docker start hadoop-master hadoop-worker1 hadoop-worker2 puis d'entrer dans le contenaire master: docker exec -it hadoop-master bash Lancer ensuite les d\u00e9mons yarn et hdfs: ./start-hadoop.sh Vous pourrez v\u00e9rifier que tous les d\u00e9mons sont lanc\u00e9s en tapant: jps . Un r\u00e9sultat semblable au suivant pourra \u00eatre visible: 880 Jps 257 NameNode 613 ResourceManager 456 SecondaryNameNode La m\u00eame op\u00e9ration sur les noeuds workers (auxquels vous acc\u00e9dez \u00e0 partir de votre machine h\u00f4te de la m\u00eame fa\u00e7on que le noeud ma\u00eetre, c'est \u00e0 dire en tapant par exemple docker exec -it hadoop-worker1 bash ) devrait donner: 176 NodeManager 65 DataNode 311 Jps Test de Spark avec Spark-Shell \u00b6 Dans le but de tester l'ex\u00e9cution de spark, commencer par cr\u00e9er un fichier file1.txt dans votre noeud master, contenant le texte suivant: Hello Spark Wordcount! Also Hello to you Hadoop :) Charger ensuite ce fichier dans HDFS: hdfs dfs -put file1.txt Erreur possible Si le message suivant s'affiche: put: `.': No such file or directory , c'est parce que l'arborescence du r\u00e9pertoire principal n'est pas cr\u00e9\u00e9e dans HDFS. Pour le faire, il suffit d'ex\u00e9cuter la commande suivante avant la commande de chargement : hadoop fs mkdir -p . Pour v\u00e9rifier que spark est bien install\u00e9, taper la commande suivante: spark-shell Vous devriez avoir un r\u00e9sultat semblable au suivant: Vous pourrez tester spark avec un code scala simple comme suit: Charger le fichier file1.txt de HDFS val lines = sc . textFile ( \"file1.txt\" ) S\u00e9parer les mots selon les caract\u00e8res d'espacement val words = lines . flatMap ( _ . split ( \"\\\\s+\" )) Appliquer un map sur les mots obtenus qui produit le couple ( <mot> , 1), puis un reduce qui permet de faire la somme des 1 des mots identiques. val wc = words . map ( w => ( w , 1 )). reduceByKey ( _ + _ ) Sauvegarder le r\u00e9sultat dans le r\u00e9pertoire file1.count de HDFS. wc . saveAsTextFile ( \"file1.count\" ) Pour afficher le r\u00e9sultat, sortir de spark-shell en cliquant sur Ctrl-C . T\u00e9l\u00e9charger ensuite le r\u00e9pertoire file1.count cr\u00e9\u00e9 dans HDFS comme suit: hdfs dfs -get file1.count Le contenu des deux fichiers part-00000 et part-00001 ressemble \u00e0 ce qui suit:","title":"Partie 3 - Spark Shell"},{"location":"tp3/#objectifs","text":"Manipulation de Spark Shell avec Scala pour lancer un traitement sur des donn\u00e9es sur HDFS.","title":"Objectifs"},{"location":"tp3/#spark","text":"","title":"Spark"},{"location":"tp3/#presentation","text":"Spark est un syst\u00e8me de traitement rapide et parall\u00e8le. Il fournit des APIs de haut niveau en Java, Scala, Python et R, et un moteur optimis\u00e9 qui supporte l'ex\u00e9cution des graphes. Il supporte \u00e9galement un ensemble d'outils de haut niveau tels que Spark SQL pour le support du traitement de donn\u00e9es structur\u00e9es, MLlib pour l'apprentissage des donn\u00e9es, GraphX pour le traitement des graphes, et Spark Streaming pour le traitment des donn\u00e9es en streaming.","title":"Pr\u00e9sentation"},{"location":"tp3/#spark-et-hadoop","text":"Spark peut s'ex\u00e9cuter sur plusieurs plateformes: Hadoop, Mesos, en standalone ou sur le cloud. Il peut \u00e9galement acc\u00e9der \u00e0 diverses sources de donn\u00e9es, comme HDFS, Cassandra, HBase et S3. Dans ce lab, nous allons ex\u00e9cuter Spark sur Hadoop YARN. YARN s'occupera ainsi de la gestion des ressources pour le d\u00e9clenchement et l'ex\u00e9cution des Jobs Spark.","title":"Spark et Hadoop"},{"location":"tp3/#installation","text":"Nous avons proc\u00e9d\u00e9 \u00e0 l'installation de Spark sur le cluster Hadoop utilis\u00e9 dans le TP1 . Suivre les \u00e9tapes d\u00e9crites dans la partie Installation du TP1 pour t\u00e9l\u00e9charger l'image et ex\u00e9cuter les trois contenaires. Si cela est d\u00e9j\u00e0 fait, il suffit de lancer vos machines gr\u00e2ce aux commandes suivantes: docker start hadoop-master hadoop-worker1 hadoop-worker2 puis d'entrer dans le contenaire master: docker exec -it hadoop-master bash Lancer ensuite les d\u00e9mons yarn et hdfs: ./start-hadoop.sh Vous pourrez v\u00e9rifier que tous les d\u00e9mons sont lanc\u00e9s en tapant: jps . Un r\u00e9sultat semblable au suivant pourra \u00eatre visible: 880 Jps 257 NameNode 613 ResourceManager 456 SecondaryNameNode La m\u00eame op\u00e9ration sur les noeuds workers (auxquels vous acc\u00e9dez \u00e0 partir de votre machine h\u00f4te de la m\u00eame fa\u00e7on que le noeud ma\u00eetre, c'est \u00e0 dire en tapant par exemple docker exec -it hadoop-worker1 bash ) devrait donner: 176 NodeManager 65 DataNode 311 Jps","title":"Installation"},{"location":"tp3/#test-de-spark-avec-spark-shell","text":"Dans le but de tester l'ex\u00e9cution de spark, commencer par cr\u00e9er un fichier file1.txt dans votre noeud master, contenant le texte suivant: Hello Spark Wordcount! Also Hello to you Hadoop :) Charger ensuite ce fichier dans HDFS: hdfs dfs -put file1.txt Erreur possible Si le message suivant s'affiche: put: `.': No such file or directory , c'est parce que l'arborescence du r\u00e9pertoire principal n'est pas cr\u00e9\u00e9e dans HDFS. Pour le faire, il suffit d'ex\u00e9cuter la commande suivante avant la commande de chargement : hadoop fs mkdir -p . Pour v\u00e9rifier que spark est bien install\u00e9, taper la commande suivante: spark-shell Vous devriez avoir un r\u00e9sultat semblable au suivant: Vous pourrez tester spark avec un code scala simple comme suit: Charger le fichier file1.txt de HDFS val lines = sc . textFile ( \"file1.txt\" ) S\u00e9parer les mots selon les caract\u00e8res d'espacement val words = lines . flatMap ( _ . split ( \"\\\\s+\" )) Appliquer un map sur les mots obtenus qui produit le couple ( <mot> , 1), puis un reduce qui permet de faire la somme des 1 des mots identiques. val wc = words . map ( w => ( w , 1 )). reduceByKey ( _ + _ ) Sauvegarder le r\u00e9sultat dans le r\u00e9pertoire file1.count de HDFS. wc . saveAsTextFile ( \"file1.count\" ) Pour afficher le r\u00e9sultat, sortir de spark-shell en cliquant sur Ctrl-C . T\u00e9l\u00e9charger ensuite le r\u00e9pertoire file1.count cr\u00e9\u00e9 dans HDFS comme suit: hdfs dfs -get file1.count Le contenu des deux fichiers part-00000 et part-00001 ressemble \u00e0 ce qui suit:","title":"Test de Spark avec Spark-Shell"},{"location":"tp4/","text":"Objectifs \u00b6 Utilisation de Spark pour lancer un traitement par lot sur un fichier sur HDFS. Pr\u00e9paration de l'environnement et Code \u00b6 Nous allons dans cette partie cr\u00e9er un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job. Cr\u00e9er un projet Maven avec IntelliJ intitul\u00e9 2-Spark-Batch , en utilisant la config suivante: GroupId : spark.mapreduce ArtifactId : wordcount-spark Version : 1 Rajouter dans le fichier pom les d\u00e9pendances n\u00e9cessaires, et indiquer la version du compilateur Java: <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.13 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.slf4j </groupId> <artifactId> slf4j-reload4j </artifactId> <version> 2.1.0-alpha1 </version> <scope> test </scope> </dependency> </dependencies> Sous le r\u00e9pertoire java, cr\u00e9er un package que vous appellerez spark.batch , et dedans, une classe appel\u00e9e WordCountTask . \u00c9crire le code suivant dans WordCountTask.java : package spark.batch ; import org.apache.spark.SparkConf ; import org.apache.spark.api.java.JavaPairRDD ; import org.apache.spark.api.java.JavaRDD ; import org.apache.spark.api.java.JavaSparkContext ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import scala.Tuple2 ; import java.util.Arrays ; import com.google.common.base.Preconditions ; public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { Preconditions . checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ] , args [ 1 ] ); } public void run ( String inputFilePath , String outputDir ) { String master = \"local[*]\" ; SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()) . setMaster ( master ); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \" \" )). iterator ()) . mapToPair ( word -> new Tuple2 <> ( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } La premi\u00e8re chose \u00e0 faire dans un programme Spark est de cr\u00e9er un objet JavaSparkContext , qui indique \u00e0 Spark comment acc\u00e9der \u00e0 un cluster. Pour cr\u00e9er ce contexte, vous aurez besoin de construire un objet SparkConf qui contient toutes les informations sur l'application. appName est le nom de l'application master est une URL d'un cluster Spark, Mesos ou YARN, ou bien une cha\u00eene sp\u00e9ciale local pour lancer le job en mode local. Warning Nous avons indiqu\u00e9 ici que notre master est local pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet d\u00e9conseill\u00e9 de la hard-coder dans le programme, il faudrait plut\u00f4t l'indiquer comme option de commande \u00e0 chaque fois que nous lan\u00e7ons le job. Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell . Test du code en local \u00b6 Pour tester le code sur votre machine, proc\u00e9der aux \u00e9tapes suivantes: Ins\u00e9rer un fichier texte de votre choix (par exemple le fameux loremipsum.txt ) dans le r\u00e9pertoire src/main/resources. Lancer le programme en utilisant les arguments suivants: Arg1 : le chemin du fichier loremipsum.txt Arg2 : le chemin d'un r\u00e9pertoire out sous resources (vous ne devez pas le cr\u00e9er) Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un r\u00e9pertoire out sera cr\u00e9\u00e9 sous resources , qui contient (entre autres) deux fichiers: part-00000, part-00001. Lancement du code sur le cluster \u00b6 Pour ex\u00e9cuter le code sur le cluster, modifier comme indiqu\u00e9 les lignes en jaune dans ce qui suit: public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ] , args [ 1 ] ); } public void run ( String inputFilePath , String outputDir ) { SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \"\\t\" )). iterator ()) . mapToPair ( word -> new Tuple2 <> ( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } Lancer ensuite une configuration de type Maven, avec la commande package . Un fichier intitul\u00e9 wordcount-spark-1.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target. Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le r\u00e9pertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans VSCode), et taper la commande suivante: docker cp target/wordcount-spark-1.jar hadoop-master:/root/wordcount-spark.jar Revenir \u00e0 votre contenaire master, et lancer un job Spark en utilisant ce fichier jar g\u00e9n\u00e9r\u00e9, avec la commande spark-submit , un script utilis\u00e9 pour lancer des applications spark sur un cluster. spark-submit --class spark.batch.WordCountTask --master local wordcount-spark.jar input/purchases.txt out-spark Nous allons lancer le job en mode local, pour commencer. Le fichier en entr\u00e9e est le fichier purchases.txt (que vous d\u00e9j\u00e0 charg\u00e9 dans HDFS dans le TP pr\u00e9c\u00e9dent), et le r\u00e9sultat sera stock\u00e9 dans un nouveau r\u00e9pertoire out-spark . Attention V\u00e9rifiez bien que le fichier purchases existe dans le r\u00e9pertoire input de HDFS (et que le r\u00e9pertoire out-spark n'existe pas)! Si ce n'est pas le cas, vous pouvez le charger avec les commandes suivantes: hdfs dfs -mkdir -p input hdfs dfs -put purchases.txt input Si tout se passe bien, vous devriez trouver, dans le r\u00e9pertoire out-spark , deux fichiers part-00000 et part-00001, qui ressemblent \u00e0 ce qui suit: Nous allons maintenant tester le comportement de spark-submit si on l'ex\u00e9cute en mode cluster sur YARN. Pour cela, ex\u00e9cuter le code suivant: spark-submit --class spark.batch.WordCountTask --master yarn --deploy-mode cluster wordcount-spark.jar input/purchases.txt out-spark2 En lan\u00e7ant le job sur Yarn, deux modes de d\u00e9ploiement sont possibles: Mode cluster : o\u00f9 tout le job s'ex\u00e9cute dans le cluster, c'est \u00e0 dire les Spark Executors (qui ex\u00e9cutent les vraies t\u00e2ches) et le Spark Driver (qui ordonnance les Executors). Ce dernier sera encapsul\u00e9 dans un YARN Application Master. Mode client : o\u00f9 Spark Driver s'ex\u00e9cute sur la machine cliente (tel que votre propre ordinateur portable). Si votre machine s'\u00e9teint, le job s'arr\u00eate. Ce mode est appropri\u00e9 pour les jobs interactifs. Si tout se passe bien, vous devriez obtenir un r\u00e9pertoire out-spark2 dans HDFS avec les fichiers usuels. En cas d'erreur: consulter les logs! En cas d'erreur ou d'interruption du job sur Yarn, vous pourrez consulter les fichiers logs pour chercher le message d'erreur (le message affich\u00e9 sur la console n'est pas assez explicite). Pour cela, sur votre navigateur, aller \u00e0 l'adresse: http://localhost:8041/logs/userlogs et suivez toujours les derniers liens jusqu'\u00e0 stderr .","title":"Partie 4 - Traitement par Lot avec Spark"},{"location":"tp4/#objectifs","text":"Utilisation de Spark pour lancer un traitement par lot sur un fichier sur HDFS.","title":"Objectifs"},{"location":"tp4/#preparation-de-lenvironnement-et-code","text":"Nous allons dans cette partie cr\u00e9er un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job. Cr\u00e9er un projet Maven avec IntelliJ intitul\u00e9 2-Spark-Batch , en utilisant la config suivante: GroupId : spark.mapreduce ArtifactId : wordcount-spark Version : 1 Rajouter dans le fichier pom les d\u00e9pendances n\u00e9cessaires, et indiquer la version du compilateur Java: <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.13 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.slf4j </groupId> <artifactId> slf4j-reload4j </artifactId> <version> 2.1.0-alpha1 </version> <scope> test </scope> </dependency> </dependencies> Sous le r\u00e9pertoire java, cr\u00e9er un package que vous appellerez spark.batch , et dedans, une classe appel\u00e9e WordCountTask . \u00c9crire le code suivant dans WordCountTask.java : package spark.batch ; import org.apache.spark.SparkConf ; import org.apache.spark.api.java.JavaPairRDD ; import org.apache.spark.api.java.JavaRDD ; import org.apache.spark.api.java.JavaSparkContext ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import scala.Tuple2 ; import java.util.Arrays ; import com.google.common.base.Preconditions ; public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { Preconditions . checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ] , args [ 1 ] ); } public void run ( String inputFilePath , String outputDir ) { String master = \"local[*]\" ; SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()) . setMaster ( master ); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \" \" )). iterator ()) . mapToPair ( word -> new Tuple2 <> ( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } La premi\u00e8re chose \u00e0 faire dans un programme Spark est de cr\u00e9er un objet JavaSparkContext , qui indique \u00e0 Spark comment acc\u00e9der \u00e0 un cluster. Pour cr\u00e9er ce contexte, vous aurez besoin de construire un objet SparkConf qui contient toutes les informations sur l'application. appName est le nom de l'application master est une URL d'un cluster Spark, Mesos ou YARN, ou bien une cha\u00eene sp\u00e9ciale local pour lancer le job en mode local. Warning Nous avons indiqu\u00e9 ici que notre master est local pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet d\u00e9conseill\u00e9 de la hard-coder dans le programme, il faudrait plut\u00f4t l'indiquer comme option de commande \u00e0 chaque fois que nous lan\u00e7ons le job. Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell .","title":"Pr\u00e9paration de l'environnement et Code"},{"location":"tp4/#test-du-code-en-local","text":"Pour tester le code sur votre machine, proc\u00e9der aux \u00e9tapes suivantes: Ins\u00e9rer un fichier texte de votre choix (par exemple le fameux loremipsum.txt ) dans le r\u00e9pertoire src/main/resources. Lancer le programme en utilisant les arguments suivants: Arg1 : le chemin du fichier loremipsum.txt Arg2 : le chemin d'un r\u00e9pertoire out sous resources (vous ne devez pas le cr\u00e9er) Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un r\u00e9pertoire out sera cr\u00e9\u00e9 sous resources , qui contient (entre autres) deux fichiers: part-00000, part-00001.","title":"Test du code en local"},{"location":"tp4/#lancement-du-code-sur-le-cluster","text":"Pour ex\u00e9cuter le code sur le cluster, modifier comme indiqu\u00e9 les lignes en jaune dans ce qui suit: public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ] , args [ 1 ] ); } public void run ( String inputFilePath , String outputDir ) { SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \"\\t\" )). iterator ()) . mapToPair ( word -> new Tuple2 <> ( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } Lancer ensuite une configuration de type Maven, avec la commande package . Un fichier intitul\u00e9 wordcount-spark-1.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target. Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le r\u00e9pertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans VSCode), et taper la commande suivante: docker cp target/wordcount-spark-1.jar hadoop-master:/root/wordcount-spark.jar Revenir \u00e0 votre contenaire master, et lancer un job Spark en utilisant ce fichier jar g\u00e9n\u00e9r\u00e9, avec la commande spark-submit , un script utilis\u00e9 pour lancer des applications spark sur un cluster. spark-submit --class spark.batch.WordCountTask --master local wordcount-spark.jar input/purchases.txt out-spark Nous allons lancer le job en mode local, pour commencer. Le fichier en entr\u00e9e est le fichier purchases.txt (que vous d\u00e9j\u00e0 charg\u00e9 dans HDFS dans le TP pr\u00e9c\u00e9dent), et le r\u00e9sultat sera stock\u00e9 dans un nouveau r\u00e9pertoire out-spark . Attention V\u00e9rifiez bien que le fichier purchases existe dans le r\u00e9pertoire input de HDFS (et que le r\u00e9pertoire out-spark n'existe pas)! Si ce n'est pas le cas, vous pouvez le charger avec les commandes suivantes: hdfs dfs -mkdir -p input hdfs dfs -put purchases.txt input Si tout se passe bien, vous devriez trouver, dans le r\u00e9pertoire out-spark , deux fichiers part-00000 et part-00001, qui ressemblent \u00e0 ce qui suit: Nous allons maintenant tester le comportement de spark-submit si on l'ex\u00e9cute en mode cluster sur YARN. Pour cela, ex\u00e9cuter le code suivant: spark-submit --class spark.batch.WordCountTask --master yarn --deploy-mode cluster wordcount-spark.jar input/purchases.txt out-spark2 En lan\u00e7ant le job sur Yarn, deux modes de d\u00e9ploiement sont possibles: Mode cluster : o\u00f9 tout le job s'ex\u00e9cute dans le cluster, c'est \u00e0 dire les Spark Executors (qui ex\u00e9cutent les vraies t\u00e2ches) et le Spark Driver (qui ordonnance les Executors). Ce dernier sera encapsul\u00e9 dans un YARN Application Master. Mode client : o\u00f9 Spark Driver s'ex\u00e9cute sur la machine cliente (tel que votre propre ordinateur portable). Si votre machine s'\u00e9teint, le job s'arr\u00eate. Ce mode est appropri\u00e9 pour les jobs interactifs. Si tout se passe bien, vous devriez obtenir un r\u00e9pertoire out-spark2 dans HDFS avec les fichiers usuels. En cas d'erreur: consulter les logs! En cas d'erreur ou d'interruption du job sur Yarn, vous pourrez consulter les fichiers logs pour chercher le message d'erreur (le message affich\u00e9 sur la console n'est pas assez explicite). Pour cela, sur votre navigateur, aller \u00e0 l'adresse: http://localhost:8041/logs/userlogs et suivez toujours les derniers liens jusqu'\u00e0 stderr .","title":"Lancement du code sur le cluster"},{"location":"tp5/","text":"Objectifs \u00b6 Utilisation de Spark pour faire un traitement en streaming sur des donn\u00e9es saisies au fur et \u00e0 mesure sur le clavier. Spark Streaming \u00b6 Spark est connu pour supporter \u00e9galement le traitement des donn\u00e9es en streaming. Les donn\u00e9es peuvent \u00eatre lues \u00e0 partir de plusieurs sources tel que Kafka, Flume, Kinesis ou des sockets TCP, et peuvent \u00eatre trait\u00e9es en utilisant des algorithmes complexes. Ensuite, les donn\u00e9es trait\u00e9es peuvent \u00eatre stock\u00e9es sur des syst\u00e8mes de fichiers, des bases de donn\u00e9es ou des dashboards. Il est m\u00eame possible de r\u00e9aliser des algorithmes de machine learning et de traitement de graphes sur les flux de donn\u00e9es. En interne, il fonctionne comme suit: Spark Streaming re\u00e7oit des donn\u00e9es en streaming et les divise en micro-batches, qui sont ensuite trait\u00e9s par le moteur de spark pour g\u00e9n\u00e9rer le flux final de r\u00e9sultats. Environnement et Code \u00b6 Nous allons commencer par tester le streaming en local, comme d'habitude. Pour cela: Commencer par cr\u00e9er un nouveau projet Maven appel\u00e9 3-Spark-Streaming , avec le fichier pom suivant: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> spark.streaming </groupId> <artifactId> stream </artifactId> <version> 1 </version> <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.13 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-streaming_2.13 </artifactId> <version> 3.5.0 </version> </dependency> </dependencies> </project> Cr\u00e9er une classe spark.streaming.Stream avec le code suivant: package spark.streaming ; import org.apache.spark.sql.Dataset ; import org.apache.spark.sql.Encoders ; import org.apache.spark.sql.SparkSession ; import org.apache.spark.sql.streaming.StreamingQuery ; import org.apache.spark.sql.streaming.StreamingQueryException ; import org.apache.spark.sql.streaming.Trigger ; import java.util.concurrent.TimeoutException ; import java.util.Arrays ; public class Stream { public static void main ( String [] args ) throws StreamingQueryException , TimeoutException { SparkSession spark = SparkSession . builder () . appName ( \"NetworkWordCount\" ) . master ( \"local[*]\" ) . getOrCreate (); // Cr\u00e9er un DataFrame repr\u00e9sentant le flux de lignes d'entr\u00e9e de la connexion \u00e0 localhost:9999 Dataset < String > lines = spark . readStream () . format ( \"socket\" ) . option ( \"host\" , \"localhost\" ) . option ( \"port\" , 9999 ) . load () . as ( Encoders . STRING ()); // Diviser les lignes en mots Dataset < String > words = lines . flatMap ( ( String x ) -> Arrays . asList ( x . split ( \" \" )). iterator (), Encoders . STRING ()); // Compter le nombre de fois chaque mot a \u00e9t\u00e9 trouv\u00e9 Dataset < org . apache . spark . sql . Row > wordCounts = words . groupBy ( \"value\" ). count (); // Lancer l'ex\u00e9cution de la requ\u00eate qui imprime les calculs en cours d'ex\u00e9cution sur la console StreamingQuery query = wordCounts . writeStream () . outputMode ( \"complete\" ) . format ( \"console\" ) . trigger ( Trigger . ProcessingTime ( \"1 second\" )) . start (); query . awaitTermination (); } } Ce code permet de calculer le nombre de mots dans un stream de donn\u00e9es (provenant du port localhost:9999) chaque seconde. Dans sa version actuelle, Spark encourage l'utilisation de Structured Streaming , une API de haut niveau qui fournit un traitement plus efficace, et qui est construite au dessus de Spark SQL, en int\u00e9grant les structures DataFrame et Dataset. Trigger Interval Dans Spark Structured Streaming, le concept de microbatch est utilis\u00e9 pour traiter les donn\u00e9es en continu par petits lots incr\u00e9mentaux. La dur\u00e9e de chaque micro-lot est configurable et d\u00e9termine la fr\u00e9quence de traitement des donn\u00e9es en continu. Cette dur\u00e9e est appel\u00e9e \"intervalle de d\u00e9clenchement\". Si vous ne sp\u00e9cifiez pas explicitement d'intervalle de d\u00e9clenchement, le trigger par d\u00e9faut est ProcessingTime(0) , qui est aussi connu comme le mode de traitement par micro-lots. Ce param\u00e8tre par d\u00e9faut signifie que Spark essaiera de traiter les donn\u00e9es aussi rapidement que possible, sans d\u00e9lai fixe entre les micro-lots. Lancement du code sur le cluster \u00b6 Nous allons directement lancer le code sur le cluster, car nous allons utiliser une petite commande utilitaire qui se trouve dans la majorit\u00e9 des syst\u00e8mes Unix-like: netcat . G\u00e9n\u00e9rer le fichier jar. Copier le fichier jar sur le contenaire master. On l'appellera stream-1.jar Installer la commande netcat sur le contenaire master comme suit: D'abord, faire un update de la liste ds packages sur votre contenaire: apt update Ensuite, installer netcat: apt install netcat Ouvrir un nouveau terminal sur votre contenaire master, et taper la commande suivante pour cr\u00e9er le stream: nc -lk 9999 Revenez au premier terminal pour lancer votre fichier Jar. L'application sera en \u00e9coute sur localhost:9999. spark-submit --class spark.streaming.Stream --master local stream-1.jar > out Commencer \u00e0 \u00e9crire des messages sur la console de votre terminal (l\u00e0 o\u00f9 vous avez lanc\u00e9 la commande nc) Nous avons utilis\u00e9 \u00e0 la fin de la commande spark-submit la cha\u00eene > out , pour indiquer que la sortie de la commande sera enregistr\u00e9e dans un fichier out. Une fois que vous aurez saisi le texte \u00e0 tester, arr\u00eater l'application (avec Ctrl-C), et afficher le contenu du fichier out . Vous trouverez normalement un r\u00e9sultat semblable au suivant:","title":"Partie 5 - Traitement Streaming avec Spark"},{"location":"tp5/#objectifs","text":"Utilisation de Spark pour faire un traitement en streaming sur des donn\u00e9es saisies au fur et \u00e0 mesure sur le clavier.","title":"Objectifs"},{"location":"tp5/#spark-streaming","text":"Spark est connu pour supporter \u00e9galement le traitement des donn\u00e9es en streaming. Les donn\u00e9es peuvent \u00eatre lues \u00e0 partir de plusieurs sources tel que Kafka, Flume, Kinesis ou des sockets TCP, et peuvent \u00eatre trait\u00e9es en utilisant des algorithmes complexes. Ensuite, les donn\u00e9es trait\u00e9es peuvent \u00eatre stock\u00e9es sur des syst\u00e8mes de fichiers, des bases de donn\u00e9es ou des dashboards. Il est m\u00eame possible de r\u00e9aliser des algorithmes de machine learning et de traitement de graphes sur les flux de donn\u00e9es. En interne, il fonctionne comme suit: Spark Streaming re\u00e7oit des donn\u00e9es en streaming et les divise en micro-batches, qui sont ensuite trait\u00e9s par le moteur de spark pour g\u00e9n\u00e9rer le flux final de r\u00e9sultats.","title":"Spark Streaming"},{"location":"tp5/#environnement-et-code","text":"Nous allons commencer par tester le streaming en local, comme d'habitude. Pour cela: Commencer par cr\u00e9er un nouveau projet Maven appel\u00e9 3-Spark-Streaming , avec le fichier pom suivant: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> spark.streaming </groupId> <artifactId> stream </artifactId> <version> 1 </version> <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.13 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-streaming_2.13 </artifactId> <version> 3.5.0 </version> </dependency> </dependencies> </project> Cr\u00e9er une classe spark.streaming.Stream avec le code suivant: package spark.streaming ; import org.apache.spark.sql.Dataset ; import org.apache.spark.sql.Encoders ; import org.apache.spark.sql.SparkSession ; import org.apache.spark.sql.streaming.StreamingQuery ; import org.apache.spark.sql.streaming.StreamingQueryException ; import org.apache.spark.sql.streaming.Trigger ; import java.util.concurrent.TimeoutException ; import java.util.Arrays ; public class Stream { public static void main ( String [] args ) throws StreamingQueryException , TimeoutException { SparkSession spark = SparkSession . builder () . appName ( \"NetworkWordCount\" ) . master ( \"local[*]\" ) . getOrCreate (); // Cr\u00e9er un DataFrame repr\u00e9sentant le flux de lignes d'entr\u00e9e de la connexion \u00e0 localhost:9999 Dataset < String > lines = spark . readStream () . format ( \"socket\" ) . option ( \"host\" , \"localhost\" ) . option ( \"port\" , 9999 ) . load () . as ( Encoders . STRING ()); // Diviser les lignes en mots Dataset < String > words = lines . flatMap ( ( String x ) -> Arrays . asList ( x . split ( \" \" )). iterator (), Encoders . STRING ()); // Compter le nombre de fois chaque mot a \u00e9t\u00e9 trouv\u00e9 Dataset < org . apache . spark . sql . Row > wordCounts = words . groupBy ( \"value\" ). count (); // Lancer l'ex\u00e9cution de la requ\u00eate qui imprime les calculs en cours d'ex\u00e9cution sur la console StreamingQuery query = wordCounts . writeStream () . outputMode ( \"complete\" ) . format ( \"console\" ) . trigger ( Trigger . ProcessingTime ( \"1 second\" )) . start (); query . awaitTermination (); } } Ce code permet de calculer le nombre de mots dans un stream de donn\u00e9es (provenant du port localhost:9999) chaque seconde. Dans sa version actuelle, Spark encourage l'utilisation de Structured Streaming , une API de haut niveau qui fournit un traitement plus efficace, et qui est construite au dessus de Spark SQL, en int\u00e9grant les structures DataFrame et Dataset. Trigger Interval Dans Spark Structured Streaming, le concept de microbatch est utilis\u00e9 pour traiter les donn\u00e9es en continu par petits lots incr\u00e9mentaux. La dur\u00e9e de chaque micro-lot est configurable et d\u00e9termine la fr\u00e9quence de traitement des donn\u00e9es en continu. Cette dur\u00e9e est appel\u00e9e \"intervalle de d\u00e9clenchement\". Si vous ne sp\u00e9cifiez pas explicitement d'intervalle de d\u00e9clenchement, le trigger par d\u00e9faut est ProcessingTime(0) , qui est aussi connu comme le mode de traitement par micro-lots. Ce param\u00e8tre par d\u00e9faut signifie que Spark essaiera de traiter les donn\u00e9es aussi rapidement que possible, sans d\u00e9lai fixe entre les micro-lots.","title":"Environnement et Code"},{"location":"tp5/#lancement-du-code-sur-le-cluster","text":"Nous allons directement lancer le code sur le cluster, car nous allons utiliser une petite commande utilitaire qui se trouve dans la majorit\u00e9 des syst\u00e8mes Unix-like: netcat . G\u00e9n\u00e9rer le fichier jar. Copier le fichier jar sur le contenaire master. On l'appellera stream-1.jar Installer la commande netcat sur le contenaire master comme suit: D'abord, faire un update de la liste ds packages sur votre contenaire: apt update Ensuite, installer netcat: apt install netcat Ouvrir un nouveau terminal sur votre contenaire master, et taper la commande suivante pour cr\u00e9er le stream: nc -lk 9999 Revenez au premier terminal pour lancer votre fichier Jar. L'application sera en \u00e9coute sur localhost:9999. spark-submit --class spark.streaming.Stream --master local stream-1.jar > out Commencer \u00e0 \u00e9crire des messages sur la console de votre terminal (l\u00e0 o\u00f9 vous avez lanc\u00e9 la commande nc) Nous avons utilis\u00e9 \u00e0 la fin de la commande spark-submit la cha\u00eene > out , pour indiquer que la sortie de la commande sera enregistr\u00e9e dans un fichier out. Une fois que vous aurez saisi le texte \u00e0 tester, arr\u00eater l'application (avec Ctrl-C), et afficher le contenu du fichier out . Vous trouverez normalement un r\u00e9sultat semblable au suivant:","title":"Lancement du code sur le cluster"}]}